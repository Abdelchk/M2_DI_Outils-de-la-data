# Exercice 04 : Apache Spark + Jupyter - Analyse Big Data

## ğŸ¯ Objectifs

- Installer Apache Spark
- Utiliser Jupyter avec Spark
- Analyser de gros volumes de donnÃ©es
- CrÃ©er des visualisations interactives
- MaÃ®triser le traitement distribuÃ©

## ğŸ“‹ PrÃ©requis

- Python 3.8+
- Java 8+ (requis pour Spark)
- 4GB RAM minimum

## ğŸ“¦ Installation

### Option 1 : Avec PySpark (RecommandÃ©)

```bash
# Installer PySpark
pip install pyspark jupyter pandas matplotlib seaborn

# VÃ©rifier l'installation
python -c "from pyspark.sql import SparkSession; print('OK')"
```

### Option 2 : TÃ©lÃ©charger Spark

```bash
# TÃ©lÃ©charger Spark depuis https://spark.apache.org/downloads.html
# Extraire et configurer
export SPARK_HOME=/chemin/vers/spark
export PATH=$PATH:$SPARK_HOME/bin
```

## ğŸ“Š DonnÃ©es

1. **GÃ©nÃ©rez les donnÃ©es** :
   ```bash
   cd exercice-04
   python generer_donnees.py
   ```

## ğŸ“ Instructions

### Ã‰tape 1 : DÃ©marrer Jupyter avec Spark

1. **CrÃ©ez un notebook Jupyter** :
   ```bash
   jupyter notebook
   ```

2. **Dans le notebook, configurez Spark** :
   ```python
   from pyspark.sql import SparkSession
   
   spark = SparkSession.builder \
       .appName("AnalyseTransactions") \
       .master("local[4]") \
       .config("spark.sql.adaptive.enabled", "true") \
       .getOrCreate()
   
   spark
   ```

### Ã‰tape 2 : Charger les donnÃ©es

1. **Chargez le CSV** :
   ```python
   df = spark.read.csv(
       "donnees/transactions_large.csv",
       header=True,
       inferSchema=True
   )
   ```

2. **Explorez les donnÃ©es** :
   - Affichez le schÃ©ma
   - Comptez le nombre de lignes
   - Affichez quelques exemples

### Ã‰tape 3 : Transformations de base

1. **Filtrage** :
   - Transactions > 100â‚¬
   - Transactions d'une pÃ©riode spÃ©cifique

2. **AgrÃ©gations** :
   - CA total par client
   - Produits les plus vendus
   - Statistiques par catÃ©gorie

3. **Fonctions de fenÃªtre** :
   - Montant cumulÃ© par client
   - Top 3 produits par catÃ©gorie

### Ã‰tape 4 : Analyses avancÃ©es

1. **Analyse temporelle** :
   - CA par mois
   - Tendances
   - SaisonnalitÃ©s

2. **Segmentation** :
   - Clients par niveau de CA
   - Produits par performance

3. **DÃ©tection d'anomalies** :
   - Transactions suspectes
   - Outliers

### Ã‰tape 5 : Visualisations

1. **Utilisez Pandas pour visualiser** :
   ```python
   # Convertir en Pandas (pour petits rÃ©sultats)
   df_pandas = resultat.toPandas()
   
   # CrÃ©er des graphiques
   import matplotlib.pyplot as plt
   import seaborn as sns
   ```

2. **CrÃ©ez au moins 3 visualisations** :
   - Graphique de tendances
   - Graphique de comparaison
   - Graphique de rÃ©partition

### Ã‰tape 6 : Export des rÃ©sultats

1. **Exportez en CSV** :
   ```python
   resultat.coalesce(1).write.csv(
       "output/resultats",
       header=True,
       mode="overwrite"
   )
   ```

2. **Exportez en Parquet** (recommandÃ©) :
   ```python
   resultat.write.parquet(
       "output/resultats_parquet",
       mode="overwrite"
   )
   ```

## ğŸ“ Structure attendue

```
exercice-04/
â”œâ”€â”€ README.md (ce fichier)
â”œâ”€â”€ donnees/
â”‚   â””â”€â”€ transactions_large.csv
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ analyse_spark.ipynb
â””â”€â”€ solutions/
    â””â”€â”€ votre-nom/
        â”œâ”€â”€ notebook.ipynb
        â”œâ”€â”€ output/ (rÃ©sultats exportÃ©s)
        â”œâ”€â”€ resultats.md
        â””â”€â”€ visualisations/ (graphiques)
```

## âœ… CritÃ¨res d'Ã©valuation

- [ ] Spark installÃ© et fonctionnel
- [ ] Notebook Jupyter crÃ©Ã©
- [ ] DonnÃ©es chargÃ©es et analysÃ©es
- [ ] Au moins 5 analyses effectuÃ©es
- [ ] Visualisations crÃ©Ã©es
- [ ] RÃ©sultats exportÃ©s
- [ ] Documentation complÃ¨te

## ğŸ’¡ Conseils

- Utilisez les DataFrames plutÃ´t que les RDD
- Ã‰vitez les collect() sur gros datasets
- Utilisez le cache judicieusement
- Testez avec de petits Ã©chantillons d'abord
- Utilisez explain() pour voir le plan d'exÃ©cution

## ğŸ“š Ressources

- Documentation Spark : https://spark.apache.org/docs/
- Guide PySpark : https://spark.apache.org/docs/latest/api/python/
- Tutoriels : https://spark.apache.org/docs/latest/quick-start.html

## ğŸ†˜ Aide

Si vous Ãªtes bloquÃ© :
1. VÃ©rifiez que Java est installÃ©
2. Consultez la documentation officielle
3. Ouvrez une issue sur le dÃ©pÃ´t GitHub

## ğŸ“¤ Comment soumettre votre solution

### Ã‰tapes pour pousser votre exercice sur GitHub

1. **GÃ©nÃ©rez les donnÃ©es** :
   ```bash
   cd exercice-04
   python generer_donnees.py
   ```

2. **CrÃ©ez votre dossier de solution** :
   ```bash
   mkdir -p solutions/votre-nom
   cd solutions/votre-nom
   ```

3. **Sauvegardez votre notebook Jupyter**
4. **Exportez vos rÃ©sultats et visualisations**
5. **CrÃ©ez un fichier `resultats.md`**

6. **Ajoutez et commitez** :
   ```bash
   git add solutions/votre-nom/
   git commit -m "Solution exercice 04 - Votre Nom"
   git push origin main
   ```

**Important** : N'oubliez pas de remplacer "votre-nom" par votre vrai nom !
