# Exercice 04 : Analyse de donnÃ©es avec Apache Spark

## ğŸ¯ Objectifs

- Comprendre les concepts du Big Data
- MaÃ®triser Apache Spark pour le traitement distribuÃ©
- ImplÃ©menter des transformations sur de gros volumes de donnÃ©es
- Optimiser les performances Spark

## ğŸ“‹ PrÃ©requis

- Python 3.8+
- BibliothÃ¨ques : pyspark, pandas
- Java 8+ (requis pour Spark)
- Connaissances en programmation distribuÃ©e

## ğŸ“¦ Installation

```bash
# Installer Java (si nÃ©cessaire)
# Windows: TÃ©lÃ©charger depuis https://adoptium.net/
# Linux/Mac: sudo apt-get install openjdk-8-jdk

pip install pyspark pandas
```

## ğŸ“ Instructions

### Contexte

Vous travaillez avec un dataset de transactions e-commerce volumineux. Vous devez utiliser Spark pour analyser ces donnÃ©es de maniÃ¨re distribuÃ©e.

### Ã‰tape 1 : Configuration Spark

1. CrÃ©ez un script `spark_config.py` qui configure Spark :
   - Mode local avec plusieurs cores
   - Configuration de la mÃ©moire
   - Optimisation des paramÃ¨tres

2. CrÃ©ez une session Spark avec les bonnes configurations

### Ã‰tape 2 : Chargement des donnÃ©es

1. Chargez le dataset de transactions depuis un fichier CSV volumineux
2. CrÃ©ez un DataFrame Spark
3. Explorez le schÃ©ma et les donnÃ©es
4. Affichez les statistiques de base

### Ã‰tape 3 : Transformations de base

1. **Filtrage** :
   - Filtrez les transactions supÃ©rieures Ã  100â‚¬
   - Filtrez par pÃ©riode (ex: derniÃ¨re annÃ©e)

2. **SÃ©lection et projection** :
   - SÃ©lectionnez les colonnes pertinentes
   - CrÃ©ez de nouvelles colonnes calculÃ©es

3. **AgrÃ©gations** :
   - Calculez le CA total par client
   - Trouvez les produits les plus vendus
   - Calculez les statistiques par catÃ©gorie

### Ã‰tape 4 : Jointures et fenÃªtres

1. **Jointures** :
   - Joignez les transactions avec une table de rÃ©fÃ©rence produits
   - Joignez avec une table clients

2. **Fonctions de fenÃªtre (Window Functions)** :
   - Calculez le montant cumulÃ© par client
   - Trouvez le top 3 des produits par catÃ©gorie
   - Calculez des moyennes mobiles

### Ã‰tape 5 : Optimisation

1. **Partitionnement** :
   - Repartitionnez les donnÃ©es de maniÃ¨re optimale
   - Utilisez le partitionnement par colonnes clÃ©s

2. **Caching** :
   - Identifiez les DataFrames Ã  rÃ©utiliser
   - Utilisez le cache Spark efficacement

3. **Broadcast joins** :
   - Utilisez les broadcast joins pour les petites tables

### Ã‰tape 6 : Analyses avancÃ©es

1. **Analyse temporelle** :
   - Analysez les tendances par mois/trimestre
   - DÃ©tectez les saisonnalitÃ©s

2. **Segmentation** :
   - Segmentez les clients (RFM analysis)
   - Identifiez les patterns de comportement

3. **DÃ©tection d'anomalies** :
   - DÃ©tectez les transactions suspectes
   - Identifiez les outliers

### Ã‰tape 7 : Export et visualisation

1. Exportez les rÃ©sultats agrÃ©gÃ©s en CSV/Parquet
2. CrÃ©ez des visualisations avec les donnÃ©es agrÃ©gÃ©es
3. Documentez vos analyses dans `resultats.md`

## ğŸ“ Structure attendue

```
exercice-04/
â”œâ”€â”€ README.md (ce fichier)
â”œâ”€â”€ donnees/
â”‚   â””â”€â”€ transactions_large.csv (gÃ©nÃ©rÃ© par le script)
â”œâ”€â”€ solutions/
â”‚   â””â”€â”€ votre-nom/
â”‚       â”œâ”€â”€ spark_config.py
â”‚       â”œâ”€â”€ analysis.py
â”‚       â”œâ”€â”€ generate_data.py (pour gÃ©nÃ©rer les donnÃ©es de test)
â”‚       â”œâ”€â”€ resultats.md
â”‚       â””â”€â”€ outputs/
â”‚           â”œâ”€â”€ ca_par_client.csv
â”‚           â””â”€â”€ produits_populaires.parquet
```

## âœ… CritÃ¨res d'Ã©valuation

- [ ] Configuration Spark optimale
- [ ] Transformations efficaces
- [ ] Utilisation appropriÃ©e des fonctions Spark
- [ ] Optimisations implÃ©mentÃ©es
- [ ] Analyses pertinentes
- [ ] Code bien documentÃ©

## ğŸ’¡ Conseils

- Utilisez `spark.sql()` pour les requÃªtes SQL complexes
- Ã‰vitez les collect() sur de gros datasets
- Utilisez les colonnes typÃ©es plutÃ´t que les RDD
- Monitorer l'UI Spark (http://localhost:4040)
- Pensez Ã  la persistance (cache, checkpoint)

## ğŸš€ Niveau avancÃ© (Bonus)

- ImplÃ©mentez un streaming avec Spark Streaming
- Utilisez MLlib pour des analyses prÃ©dictives
- DÃ©ployez sur un cluster (local ou cloud)
- CrÃ©ez des UDF (User Defined Functions) optimisÃ©es

## ğŸ“¤ Comment soumettre votre solution

### Ã‰tapes pour pousser votre exercice sur GitHub

1. **PrÃ©parez votre environnement** :
   ```bash
   cd exercice-04
   ```
   
   2. **GÃ©nÃ©rez les donnÃ©es nÃ©cessaires** (si applicable) :
   ```bash
   python generer_donnees.py
   ```

2. **CrÃ©ez votre dossier de solution** :
   ```bash
   mkdir -p solutions/votre-nom
   cd solutions/votre-nom
   ```

3. **Placez tous vos fichiers** dans ce dossier :
   - Votre code source
   - Votre fichier `resultats.md`
   - Tous les fichiers gÃ©nÃ©rÃ©s (graphiques, exports, etc.)

4. **Ajoutez et commitez vos fichiers** :
   ```bash
   git add solutions/votre-nom/
   git commit -m "Solution exercice 04 - Votre Nom"
   ```

5. **Poussez vers GitHub** :
   ```bash
   git push origin main
   ```
   
   Si vous avez forkÃ© le dÃ©pÃ´t :
   ```bash
   git push origin votre-branche
   ```

6. **CrÃ©ez une Pull Request** (si vous avez forkÃ©) ou vos fichiers seront directement visibles dans le dÃ©pÃ´t principal.

### Structure de votre soumission

Votre dossier `solutions/votre-nom/` doit contenir :
- âœ… Tous vos fichiers de code source
- âœ… `resultats.md` : Votre analyse et rÃ©sultats
- âœ… Tous les fichiers gÃ©nÃ©rÃ©s (graphiques, exports, etc.)
- âœ… Un fichier `README.md` (optionnel) expliquant votre approche

### VÃ©rification

Avant de pousser, vÃ©rifiez que :
- [ ] Votre code fonctionne sans erreur
- [ ] Tous les fichiers sont prÃ©sents
- [ ] La documentation est complÃ¨te
- [ ] Les critÃ¨res d'Ã©valuation sont remplis

**Important** : N'oubliez pas de remplacer "votre-nom" par votre vrai nom dans le chemin du dossier ! dans le README principal du dÃ©pÃ´t pour soumettre votre solution.

